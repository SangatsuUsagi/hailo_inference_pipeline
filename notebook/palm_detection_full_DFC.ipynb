{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Palm Detection Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports used throughout the tutorial\n",
    "\n",
    "# file operations\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import the ClientRunner class from the hailo_sdk_client package\n",
    "from hailo_sdk_client import ClientRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hardware architecture to be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_hw_arch = \"hailo8\"\n",
    "# For Hailo-15 devices, use 'hailo15h'\n",
    "# For Mini PCIe modules or Hailo-8R devices, use 'hailo8r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model name definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"palm_detection_full\"\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate calibration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration dataset should be preprocessed according yo the model's input requirements (in this case, 194x194), and it is recommended to have at least 1024 images and to use a GPU. In this tutorial, we use 1024 jpeg images under ```images``` directory, and convert it to npy file.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "The imagery dataset used here is a subset of the [Human Images Dataset - Men and Women](https://www.kaggle.com/datasets/snmahsa/human-images-dataset-men-and-women), available on Kaggle under the [MIT license](https://www.mit.edu/~amini/LICENSE.md).\n",
    "\n",
    "Please download the dataset, then extract and place the dataset under `notebook` directory as below. Images should be at least 1024 images.\n",
    "\n",
    "```\n",
    "notebook\n",
    "├── gender_dataset\n",
    "│   ├── men\n",
    "│   │   ├── 1.jpg\n",
    "│   │   ├── 2.jpg\n",
    "│   │   ├── 3.jpg\n",
    "│   │   ...\n",
    "│   │   ├── 512.jpg\n",
    "│   │   ├── 513.jpg\n",
    "│   │   └── 514.jpg\n",
    "│   └── women\n",
    "│       ├── 1.jpg\n",
    "│       ├── 2.jpg\n",
    "│       ├── 3.jpg\n",
    "│       ...\n",
    "│       ├── 511.jpg\n",
    "│       ├── 512.jpg\n",
    "│       └── 513.jpg\n",
    "├── models\n",
    "│   └── palm_detection_full.tflite\n",
    "├── palm_detection_full_DFC.ipynb\n",
    "└── palm_detection_full_inference.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(\n",
    "    image: np.ndarray, output_height: int, output_width: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize an image to the specified dimensions using NumPy and Pillow.\n",
    "\n",
    "    Parameters:\n",
    "    - image (np.ndarray): The input image as a NumPy array with shape [height, width, channels].\n",
    "    - output_height (int): Desired height of the output image.\n",
    "    - output_width (int): Desired width of the output image.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Processed image with dimensions [output_height, output_width, channels].\n",
    "    \"\"\"\n",
    "\n",
    "    # Resize using Pillow with bilinear interpolation (default in PIL.Image.resize)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    resized_image = pil_image.resize((output_width, output_height), resample=Image.Resampling.BILINEAR)\n",
    "\n",
    "    return np.array(resized_image).astype(np.uint8)\n",
    "\n",
    "\n",
    "def preproc_with_pad(\n",
    "    image: np.ndarray, output_height: int, output_width: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize an image to the specified dimensions using NumPy and Pillow.\n",
    "    Keep the aspect retio of the original image when resize and add pads to adjust to the output dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - image (np.ndarray): The input image as a NumPy array with shape [height, width, channels].\n",
    "    - output_height (int): Desired height of the output image.\n",
    "    - output_width (int): Desired width of the output image.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Processed image with dimensions [output_height, output_width, channels].\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Calculate the new side length for resizing while preserving aspect ratio\n",
    "    resize_side = max(h, w)\n",
    "\n",
    "    # Determine scaling factor based on aspect ratio\n",
    "    if h < w:\n",
    "        scale = output_width / w\n",
    "    else:\n",
    "        scale = output_height / h\n",
    "\n",
    "    # Resize using Pillow with bilinear interpolation (default in PIL.Image.resize)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    resized_image = pil_image.resize((new_w, new_h), resample=Image.Resampling.BILINEAR)\n",
    "\n",
    "    # Pad the image to target size with black color\n",
    "    delta_w = output_width - new_w\n",
    "    delta_h = output_height - new_h\n",
    "    \n",
    "    padding = (\n",
    "        int(delta_w // 2),\n",
    "        int(delta_h // 2),\n",
    "        int(delta_w - (delta_w // 2)),\n",
    "        int(delta_h - (delta_h // 2)),\n",
    "        )\n",
    "    cropped_image = ImageOps.expand(resized_image, padding)\n",
    "\n",
    "    return np.array(cropped_image).astype(np.uint8)\n",
    "\n",
    "\n",
    "def list_image_files(directory, extension):\n",
    "    images_list = []\n",
    "    \n",
    "    # Walk through the directory tree\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(extension):\n",
    "                full_path = os.path.join(root, file)\n",
    "                images_list.append(full_path)\n",
    "\n",
    "    return images_list\n",
    "    \n",
    "\n",
    "def process_images(directory, target_size, extension = (\".png\", \".jpg\"), with_pad=True):\n",
    "    \"\"\"\n",
    "    Process images in a specified directory and save them as a NumPy array.\n",
    "\n",
    "    This function scans the given directory for image files with specified extensions,\n",
    "    reads each image, ensures they are valid RGB images, and then processes them using\n",
    "    the preproc function. It returns all processed images as a single 4D NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory containing images.\n",
    "    - target_size (Tuple[int, int, float]): Desired size (height, width) for output images,\n",
    "      along with the resize factor.\n",
    "    - extension (Tuple[str], optional): Tuple of allowed image file extensions. Default is (\".png\", \".jpg\").\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array containing all processed images with shape\n",
    "                  [num_images, target_height, target_width, channels].\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the specified directory does not exist.\n",
    "    - ValueError: If an image in the directory is not a valid RGB image.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n",
    "\n",
    "\n",
    "    images_list = list_image_files(directory, extension)\n",
    "\n",
    "    if not images_list:\n",
    "        raise ValueError(\"No valid images found in the directory.\")\n",
    "\n",
    "    num_images = len(images_list)\n",
    "    processed_images = np.zeros((num_images, *target_size[:2], 3))\n",
    "\n",
    "    for idx, img_name in enumerate(images_list):\n",
    "        # Load and validate each image\n",
    "        valid_image_mode = {\"RGB\", \"RGBA\", \"P\", \"PA\"}\n",
    "        with Image.open(img_name) as img:\n",
    "            if img.mode not in valid_image_mode:\n",
    "                raise ValueError(f\"Image \\\"{img_name}:{img.mode}\\\" is not a valid RGB image.\")\n",
    "\n",
    "            # If palettised image, convert to RGB image\n",
    "            if img.mode == \"RGBA\" or img.mode == \"P\" or img.mode == \"PA\":\n",
    "                print(f\"Convert image \\\"{img_name}:{img.mode}\\\" to RGB image\")\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            # Convert PIL image to NumPy array\n",
    "            img_array = np.array(img)\n",
    "\n",
    "            # Process the image using preproc function\n",
    "            if with_pad:\n",
    "                processed_images[idx] = preproc_with_pad(img_array, *target_size)\n",
    "            else:\n",
    "                processed_images[idx] = preproc(img_array, *target_size)\n",
    "\n",
    "    return processed_images, images_list\n",
    "\n",
    "\n",
    "calib_dataset = []\n",
    "images_list = []\n",
    "\n",
    "try:\n",
    "    # Process images and save as NumPy array, palm detection full model input size is 192x192\n",
    "    calib_dataset, images_list = process_images(\n",
    "        \"./gender_dataset\",\n",
    "        target_size=(192, 192),\n",
    "        with_pad=False,\n",
    "    )\n",
    "    np.save(\"calib_dataset.npy\", calib_dataset)\n",
    "    num_of_calib_images = len(calib_dataset)\n",
    "    print(f\"Processed {num_of_calib_images} images saved to 'calib_dataset.npy'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processed image from one of calibration images\n",
    "img = np.array(Image.open(images_list[num_of_calib_images-1]))\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.title('Original image')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.array(calib_dataset[len(calib_dataset)-1,:,:,:], np.uint8), interpolation='nearest')\n",
    "plt.title('Preprocessed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Palm Detection Model from tflite to HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_path = f\"./models/{model_name}.tflite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tflite model from meidapipe to models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O {tflite_path} https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the model with the default arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
    "hn, npz = runner.translate_tf_model(\n",
    "    tflite_path,\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing, we received the error ```\"UnsupportedModelError: could not detect inputs to concatenate layer 'concat1' (translated from Identity1).\"``` \n",
    "\n",
    "To prevent this parsing error, assign the ending node to the convolutional layer preceding the reshape-concatenate layer.\n",
    "\n",
    "*Note:* You can use [Netron https://netron.app/](https://netron.app/) to check the model structure and node names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
    "hn, npz = runner.translate_tf_model(\n",
    "    tflite_path,\n",
    "    model_name,\n",
    "    start_node_names=[\"input_1\"],\n",
    "    end_node_names=[\n",
    "        \"model_1/model/classifier_palm_16_NO_PRUNING/BiasAdd;model_1/model/classifier_palm_16_NO_PRUNING/Conv2D;model_1/model/classifier_palm_16_NO_PRUNING/BiasAdd/ReadVariableOp/resource1\",\n",
    "        \"model_1/model/classifier_palm_8_NO_PRUNING/BiasAdd;model_1/model/classifier_palm_8_NO_PRUNING/Conv2D;model_1/model/classifier_palm_8_NO_PRUNING/BiasAdd/ReadVariableOp/resource1\",\n",
    "        \"model_1/model/regressor_palm_16_NO_PRUNING/BiasAdd;model_1/model/regressor_palm_16_NO_PRUNING/Conv2D;model_1/model/regressor_palm_16_NO_PRUNING/BiasAdd/ReadVariableOp/resource1\",\n",
    "        \"model_1/model/regressor_palm_8_NO_PRUNING/BiasAdd;model_1/model/regressor_palm_8_NO_PRUNING/Conv2D;model_1/model/regressor_palm_8_NO_PRUNING/BiasAdd/ReadVariableOp/resource1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save har model. (At this point, har contains only float32 model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.save_har(hailo_model_har_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hailo visualizer {hailo_model_har_name} --no-browser\n",
    "SVG(f\"{model_name}.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model with 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will load our parsed HAR\n",
    "runner = ClientRunner(har=hailo_model_har_name)\n",
    "# By default it uses the hw_arch that is saved on the HAR. For overriding, use the hw_arch flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** At first, you should optimize the model with ```optimization_level=0``` to check the model can be optimized without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a model script, that tells the compiler to add a normalization on the beginning\n",
    "# of the model (that is why we didn't normalize the calibration set;\n",
    "# Otherwise we would have to normalize it before using it)\n",
    "\n",
    "#calibration dataset\n",
    "calib_dataset = np.load('calib_dataset.npy')\n",
    "\n",
    "# Batch size is 8 by default\n",
    "alls_lines = [\n",
    "    \"normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\\n\",\n",
    "    \"model_optimization_flavor(optimization_level=2, compression_level=0, batch_size=8)\\n\",\n",
    "]\n",
    "\n",
    "# Load the model script to ClientRunner so it will be considered on optimization\n",
    "runner.load_model_script(\"\".join(alls_lines))\n",
    "\n",
    "# Call Optimize to perform the optimization process\n",
    "runner.optimize(calib_dataset)\n",
    "\n",
    "# Save the result state to a Quantized HAR file\n",
    "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
    "runner.save_har(quantized_model_har_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile quantized Palm Detection Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
    "runner = ClientRunner(har=quantized_model_har_path)\n",
    "# By default it uses the hw_arch that is saved on the HAR. It is not recommended to change the hw_arch after Optimization.\n",
    "\n",
    "hef = runner.compile()\n",
    "\n",
    "hef_name = f\"../hefs/{model_name}.hef\"\n",
    "with open(hef_name, \"wb\") as f:\n",
    "    f.write(hef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can compile the model without errors, you can test the compiled hef model with HailoRT in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
