{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Palm detection model on Hailo device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model name definitions for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"palm_detection_full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports used throughout the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from hailo_platform import (\n",
    "    HEF,\n",
    "    ConfigureParams,\n",
    "    FormatType,\n",
    "    HailoSchedulingAlgorithm,\n",
    "    HailoStreamInterface,\n",
    "    InferVStreams,\n",
    "    InputVStreamParams,\n",
    "    OutputVStreamParams,\n",
    "    VDevice,\n",
    ")\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Inference Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "TIMEOUT_MS = 10000  # Timeout for asynchronous operations in milliseconds\n",
    "\n",
    "\n",
    "class InferPipeline:\n",
    "    \"\"\"\n",
    "    Class to manage asynchronous and blocking inference pipelines for processing input data\n",
    "    using deep learning models stored in Hailo Executable Format (HEF).\n",
    "\n",
    "    Attributes:\n",
    "        out_results: Dictionary to store output results from asynchronous inference.\n",
    "        layer_name_u8: List of layer names that produce float32 format tensors as outputs.\n",
    "        layer_name_u16: List of layer names that produce uint16 format tensors as outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, net_path, batch_size, is_async, is_callback, layer_name_u8, layer_name_u16\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the InferPipeline class.\n",
    "\n",
    "        Args:\n",
    "            layer_name_u8 (list): Names of layers outputting float32 formatted tensors.\n",
    "            layer_name_u16 (list): Names of layers outputting uint16 formatted tensors.\n",
    "        \"\"\"\n",
    "        self.out_results = {}  # Store inference results\n",
    "        # Layers that output float32 format tensors\n",
    "        self.layer_name_u8 = layer_name_u8\n",
    "        # Layers that output uint16 format tensors\n",
    "        self.layer_name_u16 = layer_name_u16\n",
    "\n",
    "        self.configured_infer_model = None\n",
    "        self.bindings = None\n",
    "        self.job = None\n",
    "        self.is_async = is_async\n",
    "        self.is_callback = is_callback\n",
    "\n",
    "        # Create VDevice and set the parameters for scheduling algorithm\n",
    "        params = VDevice.create_params()\n",
    "        params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN\n",
    "\n",
    "        try:\n",
    "            self.vdevice = VDevice(params)\n",
    "\n",
    "            if self.is_async:\n",
    "                # Load the model onto the device\n",
    "                self.infer_model = self.vdevice.create_infer_model(net_path)\n",
    "\n",
    "                # Set batch size for inference operations on the loaded model\n",
    "                self.infer_model.set_batch_size(batch_size)\n",
    "\n",
    "                for out_name in self.infer_model.output_names:\n",
    "                    # Default output type is float32\n",
    "                    if out_name in self.layer_name_u8:\n",
    "                        self.infer_model.output(out_name).set_format_type(\n",
    "                            FormatType.UINT8\n",
    "                        )\n",
    "                    elif out_name in self.layer_name_u16:\n",
    "                        self.infer_model.output(out_name).set_format_type(\n",
    "                            FormatType.UINT16\n",
    "                        )\n",
    "                    else:\n",
    "                        self.infer_model.output(out_name).set_format_type(\n",
    "                            FormatType.FLOAT32\n",
    "                        )\n",
    "            else:\n",
    "                # Load the HEF model into device and configure it\n",
    "                self.hef = HEF(net_path)\n",
    "\n",
    "                # Configure network groups\n",
    "                configure_params = ConfigureParams.create_from_hef(\n",
    "                    hef=self.hef, interface=HailoStreamInterface.PCIe\n",
    "                )\n",
    "                self.network_group = (\n",
    "                    self.vdevice.configure(self.hef, configure_params)\n",
    "                )[0]\n",
    "\n",
    "                # Create input and output virtual stream parameters for inference\n",
    "                self.input_vstreams_params = InputVStreamParams.make(\n",
    "                    self.network_group,\n",
    "                    format_type=FormatType.UINT8,\n",
    "                )\n",
    "                self.output_vstreams_params = OutputVStreamParams.make(\n",
    "                    self.network_group, format_type=FormatType.FLOAT32\n",
    "                )\n",
    "\n",
    "            if self.is_async:\n",
    "                self.inference = lambda dataset: self.infer_async(dataset)\n",
    "            else:\n",
    "                self.inference = lambda dataset: self.infer_pipeline(dataset)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Cleans up resources by setting the configured inference model to None\n",
    "        and releasing the associated vdevice.\n",
    "\n",
    "        The method ensures that any allocated or referenced resources are\n",
    "        properly cleaned up. This is typically used as part of a teardown process\n",
    "        when an object is no longer needed, preventing resource leaks.\n",
    "        \"\"\"\n",
    "        # Reset the configured inference model to free resources\n",
    "        if self.configured_infer_model is not None:\n",
    "            self.configured_infer_model = None\n",
    "\n",
    "        # Release the vdevice resource\n",
    "        self.vdevice.release()\n",
    "\n",
    "    def callback(self, completion_info):\n",
    "        \"\"\"\n",
    "        Callback function to handle the completion of inference.\n",
    "\n",
    "        Args:\n",
    "            completion_info: Contains information about the completion status, including\n",
    "                             exceptions if any occurred during inference.\n",
    "        \"\"\"\n",
    "        if completion_info.exception:\n",
    "            # Handle exceptions that occurred during inference\n",
    "            print(f\"Inference error: {completion_info.exception}\")\n",
    "        else:\n",
    "            for out_name in self.bindings._output_names:\n",
    "                # Store results from each output layer into self.out_results\n",
    "                self.out_results[out_name] = self.bindings.output(out_name).get_buffer()\n",
    "\n",
    "    def infer_async(self, infer_inputs):\n",
    "        \"\"\"\n",
    "        Perform asynchronous inference on input data.\n",
    "\n",
    "        Args:\n",
    "            infer_inputs (list): List of input frames for the model.\n",
    "            net_path (str): Path to the HEF (Hailo Executable Format) model file.\n",
    "            batch_size (int): Number of images processed in a single batch. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "            infer_results (dict): Dictionary containing the inference results keyed by output layer names.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.configured_infer_model = self.infer_model.configure()\n",
    "\n",
    "            # Create bindings to manage input/output data buffers\n",
    "            self.bindings = self.configured_infer_model.create_bindings()\n",
    "\n",
    "            # Set input buffers using the provided infer inputs\n",
    "            for in_name, infer_input in zip(self.infer_model.input_names, infer_inputs):\n",
    "                self.bindings.input(in_name).set_buffer(infer_input)\n",
    "\n",
    "            # Allocate and set output buffers based on expected data format type\n",
    "            for out_name in self.infer_model.output_names:\n",
    "                out_buffer = np.array([])\n",
    "                # Default output type is float32\n",
    "                if out_name in self.layer_name_u8:\n",
    "                    out_buffer = np.empty(\n",
    "                        self.infer_model.output(out_name).shape,\n",
    "                        dtype=np.uint8,\n",
    "                    )\n",
    "                elif out_name in self.layer_name_u16:\n",
    "                    out_buffer = np.empty(\n",
    "                        self.infer_model.output(out_name).shape,\n",
    "                        dtype=np.uint16,\n",
    "                    )\n",
    "                else:\n",
    "                    out_buffer = np.empty(\n",
    "                        self.infer_model.output(out_name).shape,\n",
    "                        dtype=np.float32,\n",
    "                    )\n",
    "\n",
    "                self.bindings.output(out_name).set_buffer(out_buffer)\n",
    "\n",
    "            # Set timeout for async inference\n",
    "            self.configured_infer_model.wait_for_async_ready(timeout_ms=TIMEOUT_MS)\n",
    "\n",
    "            if self.is_callback:\n",
    "                # Start inference and use callback function for handling results\n",
    "                self.job = self.configured_infer_model.run_async(\n",
    "                    [self.bindings], partial(self.callback)\n",
    "                )\n",
    "            else:\n",
    "                # Start inference without callback\n",
    "                self.job = self.configured_infer_model.run_async([self.bindings])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def wait_and_get_ouput(self):\n",
    "        \"\"\"\n",
    "        Waits for an inference job to complete and collects the output results.\n",
    "\n",
    "        This function waits for a specified timeout period for the inference job\n",
    "        to finish. Once completed, it retrieves the results from the model's\n",
    "        output buffers and stores them in a dictionary keyed by output names.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are output names of the model and values\n",
    "                are the corresponding buffers containing inference results.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the waiting or result collection process,\n",
    "                    it will be caught and printed to the console.\n",
    "        \"\"\"\n",
    "        infer_results = {}\n",
    "\n",
    "        try:\n",
    "            # Wait for inference to complete to retrieve results from self.out_results\n",
    "            self.job.wait(TIMEOUT_MS)\n",
    "\n",
    "            # Collect the final results after completion\n",
    "            for index, out_name in enumerate(self.infer_model.output_names):\n",
    "                if self.is_callback:\n",
    "                    buffer = self.out_results[out_name]\n",
    "                else:\n",
    "                    # If no callback is used, manually collect results after waiting\n",
    "                    buffer = self.bindings.output(\n",
    "                        self.infer_model.output_names[index]\n",
    "                    ).get_buffer()\n",
    "\n",
    "                infer_results[out_name] = buffer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "\n",
    "        return infer_results\n",
    "\n",
    "    def infer_pipeline(self, infer_inputs):\n",
    "        \"\"\"\n",
    "        Perform blocking inference on input data using a network group configuration.\n",
    "\n",
    "        Args:\n",
    "            infer_inputs (list): List of input frames for the model.\n",
    "            net_path (str): Path to the HEF (Hailo Executable Format) model file.\n",
    "            batch_size (int): Number of images processed in a single batch. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "            infer_results (dict): Dictionary containing the inference results keyed by output layer names.\n",
    "        \"\"\"\n",
    "        infer_results = {}\n",
    "\n",
    "        try:\n",
    "            # Prepare input data by expanding dimensions for each input stream\n",
    "            input_data = {}\n",
    "            for i, input_vstream_info in enumerate(self.hef.get_input_vstream_infos()):\n",
    "                input_data[input_vstream_info.name] = infer_inputs[i][np.newaxis, :]\n",
    "\n",
    "            # Perform inference on the network group using configured virtual streams\n",
    "            with InferVStreams(\n",
    "                self.network_group,\n",
    "                self.input_vstreams_params,\n",
    "                self.output_vstreams_params,\n",
    "            ) as infer_pipeline:\n",
    "                buffer = infer_pipeline.infer(input_data)\n",
    "                for i, output_vstream_info in enumerate(\n",
    "                    self.hef.get_output_vstream_infos()\n",
    "                ):\n",
    "                    infer_results[output_vstream_info.name] = buffer[\n",
    "                        output_vstream_info.name\n",
    "                    ].squeeze()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "\n",
    "        return infer_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Inference with hef and specific image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tensor_info(name, format, order, shape):\n",
    "    \"\"\"Returns a string representation of a tensor with specified format and order.\n",
    "\n",
    "    This function generates a formatted string that describes the structure and\n",
    "    attributes of a tensor. It is particularly useful for logging or debugging purposes,\n",
    "    where understanding tensor dimensions and types may be crucial for diagnosing issues\n",
    "    in data processing pipelines or machine learning workflows.\n",
    "\n",
    "    The function takes into account different possible shapes of the tensor, supporting:\n",
    "    - 1D tensors represented simply by their size.\n",
    "    - 2D tensors described by height and width.\n",
    "    - 3D tensors extended with channel information.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The identifier or name of the tensor.\n",
    "        format: An object representing the data format. It is assumed that this can be\n",
    "                converted to a string, where only the last segment after a dot is used.\n",
    "        order: An object representing the storage order in memory. Similar to `format`, it\n",
    "               should have a string representation with meaningful segments post-dot.\n",
    "        shape (tuple): A tuple indicating the dimensions of the tensor. Only tuples\n",
    "                       of length 1, 2, or 3 are supported.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted description string that includes the name, format, order, and\n",
    "             dimensional details of the tensor.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `shape` is not a tuple with 1, 2, or 3 elements, which ensures\n",
    "                    only valid configurations are processed.\n",
    "\n",
    "    Example Use Cases:\n",
    "    - For diagnostics, when visualizing model architectures where tensor dimensions must be clear.\n",
    "    - In logging systems to record tensor metadata at various stages of data processing.\n",
    "\n",
    "    Notes on Implementation:\n",
    "    The function employs string manipulations to extract relevant segments from `format` and\n",
    "    `order`. It uses conditional checks to determine how to format the output based on the size\n",
    "    of `shape`, which allows for flexible handling of different types of tensors.\n",
    "    \"\"\"\n",
    "    if not isinstance(shape, tuple) or len(shape) not in [1, 2, 3]:\n",
    "        raise ValueError(\"tensor must be a tuple of length 1, 2 or 3\")\n",
    "\n",
    "    format = str(format).rsplit(\".\", 1)[1]\n",
    "    order = str(order).rsplit(\".\", 1)[1]\n",
    "\n",
    "    if len(shape) == 3:\n",
    "        H, W, C = shape[:3]\n",
    "        return f\"{name} {format}, {order}({H}x{W}x{C})\"\n",
    "    if len(shape) == 2:\n",
    "        H, W = shape[:2]\n",
    "        return f\"{name} {format}, {order}({H}x{W})\"\n",
    "    else:\n",
    "        C = shape[0]\n",
    "        return f\"{name} {format}, {order}({C})\"\n",
    "\n",
    "\n",
    "def preprocess_image_from_array(image_array, shape):\n",
    "    \"\"\"Resize and pad images to be input for detectors.\n",
    "\n",
    "    The face and palm detector networks take 256x256 and 128x128 images\n",
    "    as input. As such, the image is resized to fit these dimensions while maintaining aspect ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - image_array: numpy array representing the image.\n",
    "    - shape: tuple or integer specifying the target size (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - img: Resized image as a numpy array.\n",
    "    - scale: Scale factor between original and target sizes.\n",
    "    - pad: Pixels of padding applied in the original image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine input dimensions\n",
    "    height, width = image_array.shape[:2]\n",
    "    target_height, target_width = shape if isinstance(shape, tuple) else (shape, shape)\n",
    "\n",
    "    # Calculate scale for resizing\n",
    "    scale0 = width / target_width\n",
    "    scale1 = height / target_height\n",
    "\n",
    "    # Resize image using PIL's resize method with LANCZOS for high-quality downsampling\n",
    "    img_resized = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    return img_resized, (scale0, scale1), (0, 0)\n",
    "\n",
    "\n",
    "def preprocess_image_from_array_with_pad(image_array, shape):\n",
    "    \"\"\"Resize and pad images to be input for detectors.\n",
    "\n",
    "    The face and palm detector networks take 256x256 and 128x128 images\n",
    "    as input. As such, the image is resized and padded to fit these dimensions while maintaining aspect ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - image_array: numpy array representing the image.\n",
    "    - shape: tuple or integer specifying the target size (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - img: Resized and padded image as a numpy array.\n",
    "    - scale: Scale factor between original and target sizes.\n",
    "    - pad: Pixels of padding applied in the original image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine input dimensions\n",
    "    height, width = image_array.shape[:2]\n",
    "    target_height, target_width = shape if isinstance(shape, tuple) else (shape, shape)\n",
    "\n",
    "    # Convert numpy array to PIL Image if necessary\n",
    "    if isinstance(image_array, np.ndarray):\n",
    "        img = Image.fromarray(image_array)\n",
    "    else:\n",
    "        img = image_array\n",
    "\n",
    "    size0 = img.size  # (width, height)\n",
    "    \n",
    "    if height >= width:  # width <= height\n",
    "        h1 = int(target_height)\n",
    "        w1 = int((target_height / height) * width)\n",
    "        padh = 0\n",
    "        padw = int((target_width - w1) / 2)\n",
    "        scale = height / h1\n",
    "    else:  # height < width\n",
    "        w1 = int(target_width)\n",
    "        h1 = int((target_width / width) * height)\n",
    "        padh = int((target_height - h1) / 2)\n",
    "        padw = 0\n",
    "        scale = width / w1\n",
    "\n",
    "    # Resize image using PIL's resize method with LANCZOS for high-quality downsampling\n",
    "    img_resized = img.resize((w1, h1), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert resized image to numpy array for padding\n",
    "    img_array = np.array(img_resized)\n",
    "\n",
    "    # Pad the resized image to the target dimensions\n",
    "    padh1, padh2 = padh, target_height - h1 - padh\n",
    "    padw1, padw2 = padw, target_width - w1 - padw\n",
    "\n",
    "    # Pad the image using numpy's pad function\n",
    "    img_padded = np.pad(\n",
    "        img_array, ((padh1, padh2), (padw1, padw2), (0, 0)), mode='constant', constant_values=0\n",
    "    )\n",
    "\n",
    "    # Calculate padding in original scale\n",
    "    pad = (int(padh * scale), int(padw * scale))\n",
    "\n",
    "    return img_padded, (scale, scale), pad\n",
    "\n",
    "\n",
    "def format_and_print_vstream_info(vstream_infos, is_input=True):\n",
    "    \"\"\"\n",
    "    Formats and prints information about vstream objects.\n",
    "\n",
    "    This function iterates over a list of vstream objects, formats their\n",
    "    details such as name, format type, order, and shape, and prints this\n",
    "    information. It also yields the name, shape, and format type of each\n",
    "    vstream object.\n",
    "\n",
    "    Parameters:\n",
    "    - vstream_infos (list): A list of vstream objects containing attributes like\n",
    "                            name, format, and shape.\n",
    "    - is_input (bool): Flag indicating if the vstreams are inputs or outputs.\n",
    "                       Default is True (inputs).\n",
    "\n",
    "    Yields:\n",
    "    - tuple: A tuple containing the name, shape, and format type of each vstream.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each vstream object with an index\n",
    "    for i, vstream in enumerate(vstream_infos):\n",
    "        # Extract attributes from the vstream object\n",
    "        name = vstream.name\n",
    "        fmt_type = vstream.format.type\n",
    "        order = vstream.format.order\n",
    "        shape = vstream.shape\n",
    "\n",
    "        # Format the info string based on whether it's input or output\n",
    "        info = f\"{'Input' if is_input else 'Output'} #{i} {format_tensor_info(name, fmt_type, order, shape)}\"\n",
    "\n",
    "        # Print the formatted information\n",
    "        print(info)\n",
    "\n",
    "        # Yield a tuple of name, shape, and format type for further processing\n",
    "        yield name, shape, fmt_type\n",
    "\n",
    "\n",
    "def inference_with_single_image(hef_name, layer_name_u8, layer_name_u16, frame):\n",
    "    \"\"\"\n",
    "    Perform inference on a single image using the specified HEF model.\n",
    "\n",
    "    This function loads a model from the given network file path (HEF), prepares an input \n",
    "    frame by resizing and padding it to match the required input shape of the model,\n",
    "    and performs either synchronous or asynchronous inference. The results, along with\n",
    "    scaling and padding information used during preprocessing, are returned.\n",
    "    \n",
    "    Parameters:\n",
    "    - hef_name (str): Path to the HEF file containing the neural network model.\n",
    "    - layer_name_u8 (str): Name of the U8 output layer from the model.\n",
    "    - layer_name_u16 (str): Name of the U16 output layer from the model.\n",
    "    - frame: The input image in a format compatible with numpy arrays, typically \n",
    "      captured as an RGB or grayscale image.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Contains inference outputs, scale factor, and padding information used\n",
    "             during preprocessing. If no valid outputs are obtained, it returns None.\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the model does not have a single input tensor or if any \n",
    "      shapes in the inputs are invalid.\n",
    "    \"\"\"\n",
    "    input_shape = []\n",
    "    \n",
    "    try:\n",
    "        # Load the model using the provided network file path\n",
    "        hef = HEF(hef_name)\n",
    "\n",
    "        # Retrieve input vstream information from the model\n",
    "        vstream_inputs = hef.get_input_vstream_infos()\n",
    "\n",
    "        if not vstream_inputs:\n",
    "            raise ValueError(\"No input streams found in the model.\")\n",
    "\n",
    "        print(\"VStream infos(inputs):\")\n",
    "\n",
    "        # Format and print input information for debugging purposes\n",
    "        for in_name, in_shape, _ in format_and_print_vstream_info(\n",
    "            vstream_inputs, is_input=True\n",
    "        ):\n",
    "            if len(in_shape) < 2:\n",
    "                raise ValueError(f\"Invalid shape for input {in_name}: {in_shape}\")\n",
    "            # Store the first two dimensions of each input shape\n",
    "            input_shape.append(in_shape[:2])\n",
    "\n",
    "        # Retrieve output vstream information from the model\n",
    "        vstream_outputs = hef.get_output_vstream_infos()\n",
    "\n",
    "        print(\"VStream infos(outputs):\")\n",
    "\n",
    "        # Categorize layers based on their output format and store their names\n",
    "        for out_name, _, out_format in format_and_print_vstream_info(\n",
    "            vstream_outputs, is_input=False\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "        # Check input tensor is only one here\n",
    "        if len(vstream_inputs) != 1:\n",
    "            raise ValueError(\n",
    "                \"This note is only supports the model with the single input tensor. Quitting.\"\n",
    "            )\n",
    "\n",
    "        # Initialize the inference pipeline with model and processing parameters\n",
    "        batch_size = 1\n",
    "        is_async = False\n",
    "        is_callback = False\n",
    "        infer = InferPipeline(\n",
    "            hef_name, batch_size, is_async, is_callback, layer_name_u8, layer_name_u16\n",
    "        )\n",
    "\n",
    "        # Preprocess the frame by resizing and padding it according to input shape\n",
    "        image_array = np.array(frame)\n",
    "        input_frame, scale, pad = preprocess_image_from_array_with_pad(\n",
    "            image_array, input_shape[0]\n",
    "        )\n",
    "        inference_dataset = [input_frame]  # Prepare dataset for current frame\n",
    "\n",
    "        try:\n",
    "            # Perform synchronous inference with the prepared dataset\n",
    "            outputs = infer.inference(inference_dataset)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if \"infer\" in locals():\n",
    "            infer.close()\n",
    "\n",
    "    return outputs, scale, pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palm detection post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on code from :\n",
    "https://github.com/AlbertaBeef/blaze_app_python/blob/main/blaze_common/blazebase.py\n",
    "\"\"\"\n",
    "\n",
    "def calculate_scale(min_scale, max_scale, stride_index, num_strides):\n",
    "    if num_strides == 1:\n",
    "        return (max_scale + min_scale) * 0.5\n",
    "    else:\n",
    "        return min_scale + (max_scale - min_scale) * stride_index / (num_strides - 1.0)\n",
    "\n",
    "\n",
    "def generate_anchors(options):\n",
    "    strides_size = len(options[\"strides\"])\n",
    "    assert options[\"num_layers\"] == strides_size\n",
    "\n",
    "    anchors = []\n",
    "    layer_id = 0\n",
    "    while layer_id < strides_size:\n",
    "        anchor_height = []\n",
    "        anchor_width = []\n",
    "        aspect_ratios = []\n",
    "        scales = []\n",
    "\n",
    "        # For same strides, we merge the anchors in the same order.\n",
    "        last_same_stride_layer = layer_id\n",
    "        while (last_same_stride_layer < strides_size) and (\n",
    "            options[\"strides\"][last_same_stride_layer] == options[\"strides\"][layer_id]\n",
    "        ):\n",
    "            scale = calculate_scale(\n",
    "                options[\"min_scale\"],\n",
    "                options[\"max_scale\"],\n",
    "                last_same_stride_layer,\n",
    "                strides_size,\n",
    "            )\n",
    "\n",
    "            if last_same_stride_layer == 0 and options[\"reduce_boxes_in_lowest_layer\"]:\n",
    "                # For first layer, it can be specified to use predefined anchors.\n",
    "                aspect_ratios.append(1.0)\n",
    "                aspect_ratios.append(2.0)\n",
    "                aspect_ratios.append(0.5)\n",
    "                scales.append(0.1)\n",
    "                scales.append(scale)\n",
    "                scales.append(scale)\n",
    "            else:\n",
    "                for aspect_ratio in options[\"aspect_ratios\"]:\n",
    "                    aspect_ratios.append(aspect_ratio)\n",
    "                    scales.append(scale)\n",
    "\n",
    "                if options[\"interpolated_scale_aspect_ratio\"] > 0.0:\n",
    "                    scale_next = (\n",
    "                        1.0\n",
    "                        if last_same_stride_layer == strides_size - 1\n",
    "                        else calculate_scale(\n",
    "                            options[\"min_scale\"],\n",
    "                            options[\"max_scale\"],\n",
    "                            last_same_stride_layer + 1,\n",
    "                            strides_size,\n",
    "                        )\n",
    "                    )\n",
    "                    scales.append(np.sqrt(scale * scale_next))\n",
    "                    aspect_ratios.append(options[\"interpolated_scale_aspect_ratio\"])\n",
    "\n",
    "            last_same_stride_layer += 1\n",
    "\n",
    "        for i in range(len(aspect_ratios)):\n",
    "            ratio_sqrts = np.sqrt(aspect_ratios[i])\n",
    "            anchor_height.append(scales[i] / ratio_sqrts)\n",
    "            anchor_width.append(scales[i] * ratio_sqrts)\n",
    "\n",
    "        stride = options[\"strides\"][layer_id]\n",
    "        feature_map_height = int(np.ceil(options[\"input_size_height\"] / stride))\n",
    "        feature_map_width = int(np.ceil(options[\"input_size_width\"] / stride))\n",
    "\n",
    "        for y in range(feature_map_height):\n",
    "            for x in range(feature_map_width):\n",
    "                for anchor_id in range(len(anchor_height)):\n",
    "                    x_center = (x + options[\"anchor_offset_x\"]) / feature_map_width\n",
    "                    y_center = (y + options[\"anchor_offset_y\"]) / feature_map_height\n",
    "\n",
    "                    new_anchor = [x_center, y_center, 0, 0]\n",
    "                    if options[\"fixed_anchor_size\"]:\n",
    "                        new_anchor[2] = 1.0\n",
    "                        new_anchor[3] = 1.0\n",
    "                    else:\n",
    "                        new_anchor[2] = anchor_width[anchor_id]\n",
    "                        new_anchor[3] = anchor_height[anchor_id]\n",
    "                    anchors.append(new_anchor)\n",
    "\n",
    "        layer_id = last_same_stride_layer\n",
    "\n",
    "    anchors = np.asarray(anchors)\n",
    "\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def _decode_boxes(raw_boxes, anchors, configs):\n",
    "    \"\"\"Converts the predictions into actual coordinates using\n",
    "    the anchor boxes. Processes the entire batch at once.\n",
    "    \"\"\"\n",
    "    boxes = np.zeros(raw_boxes.shape)\n",
    "\n",
    "    x_center = raw_boxes[..., 0] / configs[\"x_scale\"] * anchors[:, 2] + anchors[:, 0]\n",
    "    y_center = raw_boxes[..., 1] / configs[\"y_scale\"] * anchors[:, 3] + anchors[:, 1]\n",
    "\n",
    "    w = raw_boxes[..., 2] / configs[\"w_scale\"] * anchors[:, 2]\n",
    "    h = raw_boxes[..., 3] / configs[\"h_scale\"] * anchors[:, 3]\n",
    "\n",
    "    boxes[..., 0] = y_center - h / 2.0  # ymin\n",
    "    boxes[..., 1] = x_center - w / 2.0  # xmin\n",
    "    boxes[..., 2] = y_center + h / 2.0  # ymax\n",
    "    boxes[..., 3] = x_center + w / 2.0  # xmax\n",
    "\n",
    "    for k in range(configs[\"num_keypoints\"]):\n",
    "        offset = 4 + k * 2\n",
    "        keypoint_x = (\n",
    "            raw_boxes[..., offset] / configs[\"x_scale\"] * anchors[:, 2] + anchors[:, 0]\n",
    "        )\n",
    "        keypoint_y = (\n",
    "            raw_boxes[..., offset + 1] / configs[\"y_scale\"] * anchors[:, 3]\n",
    "            + anchors[:, 1]\n",
    "        )\n",
    "        boxes[..., offset] = keypoint_x\n",
    "        boxes[..., offset + 1] = keypoint_y\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def _tensors_to_detections(raw_box_tensor, raw_score_tensor, anchors, configs):\n",
    "    \"\"\"The output of the neural network is an array of shape (b, 896, 12)\n",
    "    containing the bounding box regressor predictions, as well as an array\n",
    "    of shape (b, 896, 1) with the classification confidences.\n",
    "\n",
    "    This function converts these two \"raw\" arrays into proper detections.\n",
    "    Returns a list of (num_detections, 13) arrays, one for each image in\n",
    "    the batch.\n",
    "\n",
    "    This is based on the source code from:\n",
    "    mediapipe/calculators/tflite/tflite_tensors_to_detections_calculator.cc\n",
    "    mediapipe/calculators/tflite/tflite_tensors_to_detections_calculator.proto\n",
    "    \"\"\"\n",
    "    detection_boxes = _decode_boxes(raw_box_tensor, anchors, configs)\n",
    "\n",
    "    thresh = configs[\"score_clipping_thresh\"]\n",
    "    clipped_score_tensor = np.clip(raw_score_tensor, -thresh, thresh)\n",
    "\n",
    "    detection_scores = 1 / (1 + np.exp(-clipped_score_tensor))\n",
    "    detection_scores = np.squeeze(detection_scores, axis=-1)\n",
    "\n",
    "    # Note: we stripped off the last dimension from the scores tensor\n",
    "    # because there is only has one class. Now we can simply use a mask\n",
    "    # to filter out the boxes with too low confidence.\n",
    "    mask = detection_scores >= configs[\"min_score_thresh\"]\n",
    "\n",
    "    # Because each image from the batch can have a different number of\n",
    "    # detections, process them one at a time using a loop.\n",
    "    output_detections = []\n",
    "    for i in range(raw_box_tensor.shape[0]):\n",
    "        boxes = detection_boxes[i, mask[i]]\n",
    "\n",
    "        scores = detection_scores[i, mask[i]]\n",
    "        scores = np.expand_dims(scores, axis=-1)\n",
    "\n",
    "        boxes_scores = np.concatenate((boxes, scores), axis=-1)\n",
    "        output_detections.append(boxes_scores)\n",
    "\n",
    "    return output_detections\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\"We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.shape[0]\n",
    "    B = box_b.shape[0]\n",
    "    max_xy = np.minimum(\n",
    "        np.repeat(np.expand_dims(box_a[:, 2:], axis=1), B, axis=1),\n",
    "        np.repeat(np.expand_dims(box_b[:, 2:], axis=0), A, axis=0),\n",
    "    )\n",
    "    min_xy = np.maximum(\n",
    "        np.repeat(np.expand_dims(box_a[:, :2], axis=1), B, axis=1),\n",
    "        np.repeat(np.expand_dims(box_b[:, :2], axis=0), A, axis=0),\n",
    "    )\n",
    "    inter = np.clip((max_xy - min_xy), 0, None)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = np.repeat(\n",
    "        np.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), axis=1\n",
    "        ),\n",
    "        inter.shape[1],\n",
    "        axis=1,\n",
    "    )  # [A,B]\n",
    "    area_b = np.repeat(\n",
    "        np.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), axis=0\n",
    "        ),\n",
    "        inter.shape[0],\n",
    "        axis=0,\n",
    "    )  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def overlap_similarity(box, other_boxes):\n",
    "    \"\"\"Computes the IOU between a bounding box and set of other boxes.\"\"\"\n",
    "    return jaccard(np.expand_dims(box, axis=0), other_boxes).squeeze(0)\n",
    "\n",
    "\n",
    "def _weighted_non_max_suppression(detections, configs):\n",
    "    \"\"\"The alternative NMS method as mentioned in the BlazeFace paper:\n",
    "\n",
    "    \"We replace the suppression algorithm with a blending strategy that\n",
    "    estimates the regression parameters of a bounding box as a weighted\n",
    "    mean between the overlapping predictions.\"\n",
    "\n",
    "    The original MediaPipe code assigns the score of the most confident\n",
    "    detection to the weighted detection, but we take the average score\n",
    "    of the overlapping detections.\n",
    "\n",
    "    The input detections should be a Tensor of shape (count, 17).\n",
    "\n",
    "    Returns a list of PyTorch tensors, one for each detected face.\n",
    "\n",
    "    This is based on the source code from:\n",
    "    mediapipe/calculators/util/non_max_suppression_calculator.cc\n",
    "    mediapipe/calculators/util/non_max_suppression_calculator.proto\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "\n",
    "    output_detections = []\n",
    "\n",
    "    # Sort the detections from highest to lowest score.\n",
    "    # argsort() returns ascending order, therefore read the array from end\n",
    "    remaining = np.argsort(detections[:, configs[\"num_coords\"]])[::-1]\n",
    "\n",
    "    while len(remaining) > 0:\n",
    "        detection = detections[remaining[0]]\n",
    "\n",
    "        # Compute the overlap between the first box and the other\n",
    "        # remaining boxes. (Note that the other_boxes also include\n",
    "        # the first_box.)\n",
    "        first_box = detection[:4]\n",
    "        other_boxes = detections[remaining, :4]\n",
    "        ious = overlap_similarity(first_box, other_boxes)\n",
    "\n",
    "        # If two detections don't overlap enough, they are considered\n",
    "        # to be from different faces.\n",
    "        mask = ious > configs[\"min_suppression_threshold\"]\n",
    "        overlapping = remaining[mask]\n",
    "        remaining = remaining[~mask]\n",
    "\n",
    "        # Take an average of the coordinates from the overlapping\n",
    "        # detections, weighted by their confidence scores.\n",
    "        weighted_detection = detection.copy()\n",
    "        if len(overlapping) > 1:\n",
    "            coordinates = detections[overlapping, : configs[\"num_coords\"]]\n",
    "            scores = detections[\n",
    "                overlapping, configs[\"num_coords\"] : configs[\"num_coords\"] + 1\n",
    "            ]\n",
    "            total_score = scores.sum()\n",
    "            weighted = np.sum(coordinates * scores, axis=0) / total_score\n",
    "            weighted_detection[: configs[\"num_coords\"]] = weighted\n",
    "            weighted_detection[configs[\"num_coords\"]] = total_score / len(overlapping)\n",
    "\n",
    "        output_detections.append(weighted_detection)\n",
    "\n",
    "    return output_detections\n",
    "\n",
    "\n",
    "def postprocess(outputs, anchor_params, configs):\n",
    "    \"\"\"\n",
    "    Post-processes raw model outputs to generate filtered detections for palm positions in images.\n",
    "    \n",
    "    Parameters:\n",
    "        outputs (dict): A dictionary containing the output tensors from a neural network. \n",
    "                        Expected keys are 'palm_detection_full/conv29', 'palm_detection_full/conv34',\n",
    "                        'palm_detection_full/conv30', and 'palm_detection_full/conv35'.\n",
    "        configs (dict): Configuration settings, which include 'num_anchors' for the number of anchors,\n",
    "                        'num_coords' for the number of coordinates per anchor, and parameters needed\n",
    "                        for non-maximum suppression.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list containing filtered detections after applying weighted non-maximum suppression.\n",
    "              If no valid detections are found, returns an empty list.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Validate inputs\n",
    "    required_keys = [\"palm_detection_full/conv29\", \"palm_detection_full/conv34\",\n",
    "                     \"palm_detection_full/conv30\", \"palm_detection_full/conv35\"]\n",
    "    for key in required_keys:\n",
    "        if key not in outputs:\n",
    "            raise ValueError(f\"Missing expected output tensor: {key}\")\n",
    "    \n",
    "    # Reshape specific convolutional layers to prepare for concatenation\n",
    "    try:\n",
    "        conv2D_1 = np.reshape(outputs[\"palm_detection_full/conv29\"], (1, 864, 1))\n",
    "        conv2D_2 = np.reshape(outputs[\"palm_detection_full/conv34\"], (1, 1152, 1))\n",
    "        conv2D_3 = np.reshape(outputs[\"palm_detection_full/conv30\"], (1, 864, 18))\n",
    "        conv2D_4 = np.reshape(outputs[\"palm_detection_full/conv35\"], (1, 1152, 18))\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\"Error reshaping tensors: \" + str(e))\n",
    "\n",
    "    # Concatenate reshaped tensors along the second axis and select the first element in batch\n",
    "    out1 = np.concatenate((conv2D_2, conv2D_1), axis=1)\n",
    "\n",
    "    # Concatenate along the second axis for another set of outputs\n",
    "    out2 = np.concatenate((conv2D_4, conv2D_3), axis=1)\n",
    "\n",
    "    # Generate anchor boxes based on configuration options\n",
    "    anchors = generate_anchors(anchor_params)\n",
    "    \n",
    "    # Validate the shapes of concatenated tensors against expected configurations\n",
    "    assert out1.shape[0] == 1  # batch size must be 1\n",
    "    assert out1.shape[1] == configs[\"num_anchors\"]  # number of anchors\n",
    "    assert out1.shape[2] == 1  # single score per anchor\n",
    "\n",
    "    assert out2.shape[0] == 1  # batch size must be 1\n",
    "    assert out2.shape[1] == configs[\"num_anchors\"]  # number of anchors\n",
    "    assert out2.shape[2] == configs[\"num_coords\"]  # coordinates per anchor\n",
    "    \n",
    "    # Convert tensors to detection format using model-specific logic\n",
    "    detections = _tensors_to_detections(out2, out1, anchors, configs)\n",
    "    \n",
    "    # Apply weighted non-maximum suppression to remove overlapping detections\n",
    "    filtered_detections = []\n",
    "    for i in range(len(detections)):\n",
    "        wnms_detections = _weighted_non_max_suppression(detections[i], configs)\n",
    "        if len(wnms_detections) > 0:\n",
    "            filtered_detections.append(wnms_detections)\n",
    "    \n",
    "    # Normalize final detection list\n",
    "    if len(filtered_detections) > 0:\n",
    "        normalized_detections = np.array(filtered_detections)[0]\n",
    "    else:\n",
    "        normalized_detections = []\n",
    "    \n",
    "    return normalized_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_detections(detections, scale, pad, configs):\n",
    "    \"\"\"maps detection coordinates from [0,1] to image coordinates\n",
    "\n",
    "    The palm detector networks take 192x192 images as input.\n",
    "    As such the input image is padded and resized to fit the\n",
    "    size while maintaing the aspect ratio. This function maps the\n",
    "    normalized coordinates back to the original image coordinates.\n",
    "\n",
    "    Inputs:\n",
    "        detections: nxm tensor. n is the number of detections.\n",
    "            m is 4+2*k where the first 4 valuse are the bounding\n",
    "            box coordinates and k is the number of additional\n",
    "            keypoints output by the detector.\n",
    "        scale: scalar that was used to resize the image\n",
    "        pad: padding in the x and y dimensions\n",
    "\n",
    "    \"\"\"\n",
    "    detections[:, 0] = detections[:, 0] * scale * configs[\"x_scale\"] - pad[0]\n",
    "    detections[:, 1] = detections[:, 1] * scale * configs[\"x_scale\"] - pad[1]\n",
    "    detections[:, 2] = detections[:, 2] * scale * configs[\"x_scale\"] - pad[0]\n",
    "    detections[:, 3] = detections[:, 3] * scale * configs[\"x_scale\"] - pad[1]\n",
    "\n",
    "    detections[:, 4::2] = detections[:, 4::2] * scale * configs[\"x_scale\"] - pad[1]\n",
    "    detections[:, 5::2] = detections[:, 5::2] * scale * configs[\"x_scale\"] - pad[0]\n",
    "    return detections\n",
    "\n",
    "\n",
    "def detection2roi(detection, configs):\n",
    "    \"\"\"Convert detections from detector to an oriented bounding box.\n",
    "\n",
    "    Adapted from:\n",
    "    # mediapipe/modules/face_landmark/face_detection_front_detection_to_roi.pbtxt\n",
    "\n",
    "    The center and size of the box is calculated from the center\n",
    "    of the detected box. Rotation is calcualted from the vector\n",
    "    between kp1 and kp2 relative to theta0. The box is scaled\n",
    "    and shifted by dscale and dy.\n",
    "\n",
    "    \"\"\"\n",
    "    # compute box center and scale\n",
    "    # use mediapipe/calculators/util/detections_to_rects_calculator.cc\n",
    "    xc = (detection[:, 1] + detection[:, 3]) / 2\n",
    "    yc = (detection[:, 0] + detection[:, 2]) / 2\n",
    "    scale = detection[:, 3] - detection[:, 1]  # assumes square boxes\n",
    "\n",
    "    yc += configs[\"dy\"] * scale\n",
    "    scale *= configs[\"dscale\"]\n",
    "\n",
    "    # compute box rotation\n",
    "    x0 = detection[:, 4 + 2 * configs[\"kp1\"]]\n",
    "    y0 = detection[:, 4 + 2 * configs[\"kp1\"] + 1]\n",
    "    x1 = detection[:, 4 + 2 * configs[\"kp2\"]]\n",
    "    y1 = detection[:, 4 + 2 * configs[\"kp2\"] + 1]\n",
    "    theta = np.arctan2(y0 - y1, x0 - x1) - configs[\"theta0\"]\n",
    "\n",
    "    return xc, yc, scale, theta\n",
    "\n",
    "\n",
    "def extract_roi(xc, yc, theta, scale):\n",
    "    # Assuming scale is a NumPy array of size [N]\n",
    "    scaleN = scale.reshape(-1, 1, 1).astype(np.float32)\n",
    "\n",
    "    # Define points\n",
    "    points = np.array([[-1, -1, 1, 1], [-1, 1, -1, 1]], dtype=np.float32)\n",
    "\n",
    "    # Element-wise multiplication\n",
    "    points = points * scaleN / 2\n",
    "    points = points.astype(np.float32)\n",
    "\n",
    "    R = np.zeros((theta.shape[0], 2, 2), dtype=np.float32)\n",
    "    for i in range(theta.shape[0]):\n",
    "        R[i, :, :] = [\n",
    "            [np.cos(theta[i]), -np.sin(theta[i])],\n",
    "            [np.sin(theta[i]), np.cos(theta[i])],\n",
    "        ]\n",
    "\n",
    "    center = np.column_stack((xc, yc))\n",
    "    center = np.expand_dims(center, axis=-1)\n",
    "\n",
    "    points = np.matmul(R, points) + center\n",
    "    points = points.astype(np.float32)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def draw_detections(image, filtered_detections, scale, pad, configs):\n",
    "    \"\"\"\n",
    "    Draws the filtered detections on an image by marking detected palms and keypoints.\n",
    "    \n",
    "    Parameters:\n",
    "        filtered_detections (list): A list of detection coordinates after non-maximum suppression.\n",
    "        image (PIL.Image): The image on which to draw the detections.\n",
    "        scale (float): Scaling factor applied to detection coordinates.\n",
    "        pad (tuple or list): Padding values for scaling and offset adjustments.\n",
    "        configs (dict): Configuration settings, including 'num_coords' used for keypoint calculations.\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The input image with drawn detections.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize drawing context\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    if len(filtered_detections) > 0:\n",
    "        # Denormalize detection coordinates to fit the original image scale and padding\n",
    "        detections = denormalize_detections(filtered_detections, scale, pad, configs)\n",
    "\n",
    "        for i in range(detections.shape[0]):\n",
    "            ymin, xmin, ymax, xmax = detections[i, :4]\n",
    "            \n",
    "            # Draw bounding box around detected palm\n",
    "            points = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "            draw.rectangle(points, fill=None, outline=(255, 0, 0), width=4)\n",
    "        \n",
    "            n_keypoints = detections.shape[1] // 2 - 2\n",
    "            for k in range(n_keypoints):\n",
    "                kp_x = int(detections[i, 4 + k*2])\n",
    "                kp_y = int(detections[i, 4 + k*2 + 1])\n",
    "                radius = 10\n",
    "                \n",
    "                # Calculate bounding box from center and radius to draw keypoints as circles\n",
    "                bounding_box = (kp_x - radius, kp_y - radius, kp_x + radius, kp_y + radius)\n",
    "                draw.ellipse(bounding_box, outline=(255, 0, 0), width=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_roi(image, filtered_detections, model_configs):\n",
    "    # Initialize drawing context\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    xc, yc, scale, theta = detection2roi(filtered_detections, model_configs)\n",
    "    roi_box = extract_roi(xc, yc, theta, scale)\n",
    "    \n",
    "    for i in range(roi_box.shape[0]):\n",
    "        (x1,x2,x3,x4), (y1,y2,y3,y4) = roi_box[i]\n",
    "        draw.line(((int(x1), int(y1)), (int(x2), int(y2))), fill=(0, 0, 255), width=3)\n",
    "        draw.line(((int(x1), int(y1)), (int(x3), int(y3))), fill=(0, 0, 255), width=3)\n",
    "        draw.line(((int(x2), int(y2)), (int(x4), int(y4))), fill=(0, 0, 255), width=3)\n",
    "        draw.line(((int(x3), int(y3)), (int(x4), int(y4))), fill=(0, 0, 255), width=3)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference : https://github.com/google/mediapipe/blob/v0.10.9/mediapipe/modules/palm_detection/palm_detection_gpu.pbtxt\n",
    "anchor_options = {\n",
    "    \"num_layers\": 4,\n",
    "    \"min_scale\": 0.1484375,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"input_size_height\": 192,\n",
    "    \"input_size_width\": 192,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [8, 16, 16, 16],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"num_classes\": 1,\n",
    "    \"num_anchors\": 2016,\n",
    "    \"num_coords\": 18,\n",
    "    \"score_clipping_thresh\": 100.0,\n",
    "    \"x_scale\": 192.0,\n",
    "    \"y_scale\": 192.0,\n",
    "    \"h_scale\": 192.0,\n",
    "    \"w_scale\": 192.0,\n",
    "    \"min_score_thresh\": 0.5,\n",
    "    \"min_suppression_threshold\": 0.3,\n",
    "    \"num_keypoints\": 7,\n",
    "    \"kp1\": 0,\n",
    "    \"kp2\": 2,\n",
    "    \"theta0\": np.pi / 2,\n",
    "    \"dscale\": 2.6,\n",
    "    \"dy\": -0.5,\n",
    "    \"resolution\": 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for detecting palms with test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 8-bit quantized hef with the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please prepare an appropriate image and set the image file path to `input_image` of the below cell. (Ideally, it should feature a palm.)\n",
    "\n",
    "Upon executing the cell, the hand in the picture is expected to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically, outputs are retrieved in the Float32 format and de-quantized by HailoRT.\n",
    "# However, if you need to extract outputs as INT8 or INT16, you can specify the layer name here.\n",
    "\n",
    "layer_name_u8 = []  # Layers that output float32 format tensors\n",
    "layer_name_u16 = []  # Layers that output uint16 format tensors\n",
    "\n",
    "# https://www.istockphoto.com/en/photo/brunette-hispanic-girl-showing-and-pointing-up-with-fingers-number-ten-while-smiling-gm1042593516-279105620\n",
    "input_image = \"./iStock-1042593516.jpg\"\n",
    "\n",
    "frame = Image.open(input_image)\n",
    "    \n",
    "# Run inference from image\n",
    "outputs, scale1, pad1 = inference_with_single_image(f\"../hefs/{model_name}.hef\", layer_name_u8, layer_name_u16, frame)\n",
    "\n",
    "# Post process and draw the results if palm is detected within the image\n",
    "if len(outputs) != 0:\n",
    "    filtered_detections = postprocess(outputs, anchor_options, model_configs)\n",
    "    image_output = draw_detections(frame, filtered_detections, scale1[0], pad1, model_configs)\n",
    "\n",
    "    plt.imshow(image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw ROI\n",
    "if len(outputs) != 0:\n",
    "    image_roi = draw_roi(frame, filtered_detections, model_configs)\n",
    "    plt.imshow(image_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
